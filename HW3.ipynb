{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiinpfaQMsWj"
   },
   "source": [
    "# Part 1: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEgNlH_QMsWj"
   },
   "source": [
    "For this assignment, we will implement Linear Regression as learned in class. We will use an analytical approach and a gradient descent approach for this assignment, respectively."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q2ovhfRBMsWj"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GxJ2W54bMsWk"
   },
   "source": [
    "# Load the diabetes dataset\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "print(\"features:\", X.shape)\n",
    "print(\"labels:\", y.shape)\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# TODO: Split the dataset into training and testing sets\n",
    "# You may use train_test_split in sklearn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 42)\n",
    "print(\"training data:\", X_train.shape, y_train.shape)\n",
    "print(\"test data:\", X_test.shape,  y_test.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analytical Approach\n",
    "\n",
    "For this part, you will use the analytical approach to implement a linear regression model, where the model weights are directly computed from the training data. Please refer to the course slides about how to obtain the weights."
   ],
   "metadata": {
    "id": "B7y1XlyjLvrU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: Create a bias vector and integrate it to the input matrix\n",
    "bias_train = np.ones((X_train.shape[0],1))\n",
    "bias_test = np.ones((X_test.shape[0],1))\n",
    "\n",
    "X_train_w_bias = np.hstack((bias_train, X_train))\n",
    "X_test_w_bias = np.hstack((bias_test, X_test))\n",
    "\n",
    "# TODO: Compute the weights for the linear regression model\n",
    "weights_w_bias = np.linalg.inv(X_train_w_bias.T.dot(X_train_w_bias)).dot(X_train_w_bias.T).dot(y_train)\n",
    "\n",
    "# TODO: Make predictions on the test data using the weights\n",
    "y_pred = X_test_w_bias @ weights_w_bias\n",
    "\n",
    "# TODO: Evaluate the predicted results using Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ],
   "metadata": {
    "id": "hxg3jPAXNkZA"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: Visualize the model's performance with the regression line\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6, color=\"blue\", label=\"Predictions\")\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label=\"Ideal Fit\")\n",
    "plt.xlabel(\"Actual Diabetes Progression\")\n",
    "plt.ylabel(\"Predicted Diabetes Progression\")\n",
    "plt.title(\"Actual vs. Predicted Diabetes Progression\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "OP0P79CYRTJo"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Separate and print the model weight parameters\n",
    "# weights (coefficients)\n",
    "weights = weights_w_bias[1:]\n",
    "# bias( intercept)\n",
    "bias = weights_w_bias[0]\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Bias:\", bias)\n",
    "\n",
    "# TODO: Plot a histogram chart to visualize the parameters\n",
    "plt.figure(figsize=(8, 3))\n",
    "x = np.arange(len(weights))\n",
    "plt.bar(x, weights, edgecolor = \"black\", width = 0.6)\n",
    "plt.xticks(x, data.feature_names, rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.title(\"Linear Regression Coefficients by Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Show the most and least important parameters\n",
    "important = np.abs(weights)\n",
    "most_param = np.argmax(important)\n",
    "least_param = np.argmin(important)\n",
    "print (\"Most important: feature = \", feature_names[most_param],\", coef =\", weights[most_param])\n",
    "print (\"Least important: feature = \", feature_names[least_param],\", coef =\", weights[least_param])\n"
   ],
   "metadata": {
    "id": "YeONpYfdRa1b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Descent Approach\n",
    "\n",
    "For this part, you will use a gradient descent approach to implement a linear regression model for the same diabetes dataset. Please refer to the course slides about the gradient calculation and update."
   ],
   "metadata": {
    "id": "bslmmozDMHfk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Rescale the inputs and outputs\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_mean = y_train.mean()\n",
    "y_std  = y_train.std()\n",
    "y_train_scaled = (y_train - y_mean) / y_std\n",
    "# TODO: Determine learning rate, and training epochs\n",
    "lr = 1e-4       # Try options from 0.1 to 1e-4\n",
    "num_steps = 15   # Try options from 5 to 20\n",
    "\n",
    "# TODO: Initialize model parameters\n",
    "weights = np.zeros(X_train_scaled.shape[1])\n",
    "bias = 0"
   ],
   "metadata": {
    "id": "vnNojmdQKPaL"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Model training\n",
    "for step in range(num_steps):\n",
    "  # TODO: Compute gradients on weights and bias\n",
    "  y_pred_scaled = X_train_scaled @ weights + bias\n",
    "  y_pred = y_pred_scaled * y_std + y_mean\n",
    "  error = y_train - y_pred\n",
    "  grad_weights = -(2/X_train_scaled.shape[0]) * (X_train_scaled.T @ error)\n",
    "  grad_bias = -(2/X_train_scaled.shape[0]) * np.sum(error)\n",
    "\n",
    "  # TODO: Apply gradient descent on the weights using learning rate\n",
    "  weights = weights - lr * grad_weights\n",
    "  bias = bias - lr * grad_bias\n",
    "\n",
    "  # TODO: Calculate Mean Squared Error during training\n",
    "  mse = mean_squared_error(y_train,y_pred)\n",
    "  print(\"Training error:\", mse)"
   ],
   "metadata": {
    "id": "jO3kf03VKKJb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: Make predictions on the test data\n",
    "y_pred = X_test_scaled @ weights + bias\n",
    "\n",
    "# TODO: Calculate Mean Squared Error for evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "# Print the evaluation metrics\n",
    "print(\"Mean Squared Error:\", mse)"
   ],
   "metadata": {
    "id": "lpR4zdRKKHS4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vr_MM969MsWk"
   },
   "source": [
    "# TODO: Visualize the model's performance with the regression line\n",
    "plt.figure(figsize=(8, 3))\n",
    "y_pred_test = X_test_scaled @ weights + bias\n",
    "# Scatter plot of actual vs predicted\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.6, color=\"blue\", label=\"Predictions\")\n",
    "\n",
    "# Predicted == Actual\n",
    "lo = min(y_test.min(), y_pred_test.min())\n",
    "hi = max(y_test.max(), y_pred_test.max())\n",
    "plt.plot([lo, hi], [lo, hi], \"r--\", linewidth = 2, label=\"Ideal(y=x)\")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Actual Diabetes Progression\")\n",
    "plt.ylabel(\"Predicted Diabetes Progression\")\n",
    "plt.title(\"Actual vs. Predicted Diabetes Progression\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Print the model weight parameters\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Bias:\", bias)\n",
    "\n",
    "# TODO: Plot a histogram chart to visualize the parameters\n",
    "plt.figure(figsize=(8, 3))\n",
    "x = np.arange(len(weights))\n",
    "plt.bar(x, weights, edgecolor = \"black\", width = 0.6)\n",
    "plt.xticks(x, data.feature_names, rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.title(\"Linear Regression Coefficients by Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# TODO: Show the most and least important parameters\n",
    "important = np.abs(weights)\n",
    "most_param = np.argmax(important)\n",
    "least_param = np.argmin(important)\n",
    "print (\"Most important: feature = \", feature_names[most_param],\", coef =\", weights[most_param])\n",
    "print (\"Least important: feature = \", feature_names[least_param],\", coef =\", weights[least_param])"
   ],
   "metadata": {
    "id": "Z63ZKZvDw6Eq"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use a new dataset california housing to train a linear regression model\n",
    "\n",
    "* Load the dataset from fetch_california_housing [1 point]\n",
    "* Train a linear regression model using gradient descent [4 points]\n",
    "* Report the model's performance on the test set [2 points].\n",
    "* For this dataset, how do the data split (try 20/80 and 50/50 training/test split ratios) and hyper-parameters (learning rate, training epochs) affect the outcome? Show and interpret your results [6 points].\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "4icSyULZ0bj6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# TODO: Load the dataset, train a linear regression model using gradient descent,\n",
    "# and evaluate the model's performance on the test set\n",
    "\n",
    "# Load datasets\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "Y = data.target\n",
    "print(\"features:\", X.shape)\n",
    "print(\"labels:\", Y.shape)\n",
    "\n",
    "# Train/test split\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=50  # 20/80\n",
    ")\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(\n",
    "    X, Y, test_size=0.5, random_state=42  # 50/50\n",
    ")\n",
    "\n",
    "# Split 1: 20/80\n",
    "# Scale inputs and outputs\n",
    "x_scaler_1 = StandardScaler()\n",
    "X_train_1_s = x_scaler_1.fit_transform(X_train_1)\n",
    "X_test_1_s = x_scaler_1.transform(X_test_1)\n",
    "\n",
    "y_scaler_1 = StandardScaler()\n",
    "y_train_1_s = y_scaler_1.fit_transform(y_train_1.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Initialize parameters\n",
    "n_features = X_train_1_s.shape[1]\n",
    "w1 = np.zeros(n_features)\n",
    "b1 = 0.0\n",
    "\n",
    "# GD hyperparams\n",
    "lr = 0.05\n",
    "epochs = 500\n",
    "\n",
    "# Training loop\n",
    "m1 = X_train_1_s.shape[0]\n",
    "for step in range(epochs):\n",
    "    y_pred_1_s = X_train_1_s @ w1 + b1\n",
    "    err1 = y_train_1_s - y_pred_1_s\n",
    "    grad_w1 = -(2 / m1) * (X_train_1_s.T @ err1)\n",
    "    grad_b1 = -(2 / m1) * np.sum(err1)\n",
    "    w1 -= lr * grad_w1\n",
    "    b1 -= lr * grad_b1\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test_1_s = X_test_1_s @ w1 + b1\n",
    "y_pred_test_1 = y_scaler_1.inverse_transform(y_pred_test_1_s.reshape(-1, 1)).reshape(-1)\n",
    "mse_1 = mean_squared_error(y_test_1, y_pred_test_1)\n",
    "print(f\"[20/80] Test MSE: {mse_1:.4f}\")\n",
    "\n",
    "# Plot Actual vs Predicted for split 1\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.scatter(y_test_1, y_pred_test_1, alpha=0.5, label=\"Predictions\")\n",
    "lo1 = min(y_test_1.min(), y_pred_test_1.min())\n",
    "hi1 = max(y_test_1.max(), y_pred_test_1.max())\n",
    "plt.plot([lo1, hi1], [lo1, hi1], \"r--\", linewidth=2, label=\"Ideal (y=x)\")\n",
    "plt.xlabel(\"Actual Median Value ($100k)\")\n",
    "plt.ylabel(\"Predicted Median Value ($100k)\")\n",
    "plt.title(f\"California Housing — Actual vs Predicted (20/80) | lr={lr}, epochs={epochs}\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Split 2: 50/50\n",
    "# Scale features and target (fit on training only)\n",
    "x_scaler_2 = StandardScaler()\n",
    "X_train_2_s = x_scaler_2.fit_transform(X_train_2)\n",
    "X_test_2_s = x_scaler_2.transform(X_test_2)\n",
    "\n",
    "y_scaler_2 = StandardScaler()\n",
    "y_train_2_s = y_scaler_2.fit_transform(y_train_2.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# Initialize parameters\n",
    "n_features = X_train_2_s.shape[1]\n",
    "w2 = np.zeros(n_features)\n",
    "b2 = 0.0\n",
    "\n",
    "# Training loop\n",
    "m2 = X_train_2_s.shape[0]\n",
    "for step in range(epochs):\n",
    "    y_pred_2_s = X_train_2_s @ w2 + b2\n",
    "    err2 = y_train_2_s - y_pred_2_s\n",
    "    grad_w2 = -(2 / m2) * (X_train_2_s.T @ err2)\n",
    "    grad_b2 = -(2 / m2) * np.sum(err2)\n",
    "    w2 -= lr * grad_w2\n",
    "    b2 -= lr * grad_b2\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test_2_s = X_test_2_s @ w2 + b2\n",
    "y_pred_test_2 = y_scaler_2.inverse_transform(y_pred_test_2_s.reshape(-1, 1)).reshape(-1)\n",
    "mse_2 = mean_squared_error(y_test_2, y_pred_test_2)\n",
    "print(f\"[50/50] Test MSE: {mse_2:.4f}\")\n",
    "\n",
    "# Plot Actual vs Predicted for split 2\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.scatter(y_test_2, y_pred_test_2, alpha=0.5, label=\"Predictions\")\n",
    "lo2 = min(y_test_2.min(), y_pred_test_2.min())\n",
    "hi2 = max(y_test_2.max(), y_pred_test_2.max())\n",
    "plt.plot([lo2, hi2], [lo2, hi2], \"r--\", linewidth=2, label=\"Ideal (y=x)\")\n",
    "plt.xlabel(\"Actual Median Value ($100k)\")\n",
    "plt.ylabel(\"Predicted Median Value ($100k)\")\n",
    "plt.title(f\"California Housing — Actual vs Predicted (50/50) | lr={lr}, epochs={epochs}\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "rY63GHg103mX"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRLUVZL_MsWh"
   },
   "source": [
    "# Part 2: Getting Familiar with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrZYM1lpMsWi"
   },
   "source": [
    "In this section, you will learn to use essential PyTorch functions.\n",
    "\n",
    "Make sure you have the library installed. Run the cell below to check and install them if needed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E8Od36lIMsWi"
   },
   "source": [
    "# Check and install required libraries\n",
    "!pip install numpy pandas matplotlib scikit-learn torch --quiet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.manual_seed(0)"
   ],
   "metadata": {
    "id": "Oktul32D3Uiy"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnKIQIZjMsWi"
   },
   "source": [
    "## PyTorch Tensor Construction\n",
    "\n",
    "Let's start with some basic PyTorch tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TnfLiL_9MsWj"
   },
   "source": [
    "# Task 1: Create tensors with data\n",
    "t1 = torch.ones(5,3) # A 2-D tensor with values of all ones and size of 5x3\n",
    "t2 = torch.zeros(5,3) # A 2-D tensor with values of all zeros and size of 5x3\n",
    "t3 = torch.eye(3) # A 2-D tensor of an identity matrix with size of 3x3\n",
    "t4 = torch.rand(3,4) # A 2-D tensor with random values and size of 3x4\n",
    "t5 = torch.arange(7)# A 1-D tensor with values from [0, 7) with size of 7.\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "print(t4)\n",
    "print(t5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Task 2: Convert tensors from existing data and to numpy arrays\n",
    "t1 = torch.tensor([1,2,3,4])  # Create a tensor from Python list [1,2,3,4]\n",
    "t2 = torch.tensor(np.array([1,2,3,4])) # Create a tensor from numpy array np.array([1,2,3,4])\n",
    "t3 = t2.clone() # Clone a tensor from an existing tensor t2\n",
    "t4 = t3.numpy() # Convert tensor t3 to a numpy array\n",
    "t5 = t3.to(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Move tensor t4 to GPU\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "print(t4)\n",
    "print(t5)"
   ],
   "metadata": {
    "id": "YVnn34ox7mG_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pivoting and Reshaping tensors\n",
    "In the following section we cover common methods used to pivot and reshape tensors, namely:\n",
    "1. Flatten\n",
    "1. Squeeze\n",
    "1. Reshape\n",
    "1. Transpose"
   ],
   "metadata": {
    "id": "qvUU5AEK90Bl"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_KpqeMe6MsWj"
   },
   "source": [
    "t = torch.rand(size=(3,4,5))\n",
    "print(t)\n",
    "\n",
    "# Task 3: Pivot and reshape tensors\n",
    "t1 = t.flatten() # Flatten tensor t\n",
    "t2 = t.unsqueeze(0) # Add a new dimension to t at dimension 0\n",
    "t3 = t.squeeze() # Remove the dimension in t with size of 1\n",
    "t4 = t.reshape(12,5) # Reshape t to size of 12x5\n",
    "t5 =t.transpose(0,1) # Transpose tensor t\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "print(t4)\n",
    "print(t5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensor Stack and Repeat\n",
    "1. Cat\n",
    "2. Stack\n",
    "3. Repeat"
   ],
   "metadata": {
    "id": "eTHYwwc2_yk2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tx = torch.rand(size=(3,4))\n",
    "ty = torch.rand(size=(3,4))\n",
    "\n",
    "# Task 4: Stack and repeat tensors\n",
    "t1 = torch.cat([tx, ty],dim = 1) # Concatenate tx and ty at dimension 1\n",
    "t2 = torch.cat([tx, ty], dim = 0) # Stack tx and ty at dimension 0\n",
    "t3 = tx.repeat(1,3) # Repeat tx for 3 times at dimension 1\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ],
   "metadata": {
    "id": "YYpkeMvv_04s"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mathematical Operations\n",
    "1. Point-wise/element-wise operations\n",
    "1. Redution operations\n",
    "1. Vector/Matrix operations"
   ],
   "metadata": {
    "id": "BlsKQOABDP4v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tx = torch.rand(size=(3,4))\n",
    "ty = torch.rand(size=(3,4))\n",
    "tz = torch.randn(4)\n",
    "\n",
    "# Task 5: Math operations\n",
    "t1 = tx*ty # Element-wise multiplication of tx and ty\n",
    "t2 = torch.matmul(tx,tz) # Matrix multiplication of tx and tz\n",
    "t3 = torch.sum(ty,dim=1)  # Calculate the sum of ty along dimension 1\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ],
   "metadata": {
    "id": "YguQUY4RDToV"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autograd\n",
    "(Some of the content is borrowed from the PyTorch website on autograd: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
    "`torch.autograd` is PyTorch’s automatic differentiation engine that computes gradients."
   ],
   "metadata": {
    "id": "8EgMtGUqEyHL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Task 6.1: Compute gradient for scalar inputs\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "\n",
    "# TODO - compute the gradient\n",
    "y.backward()\n",
    "\n",
    "\n",
    "print(x.grad)\n",
    "print(2*x == x.grad)\n",
    "\n",
    "# Task 6.2: Compute gradient for vectors\n",
    "x = torch.tensor([2., 3.], requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "# TODO - compute the gradient\n",
    "y.sum().backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(2*x == x.grad)"
   ],
   "metadata": {
    "id": "HSnJTgOcE8rJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`torch.autograd` tracks operations on all tensors which have their `requires_grad` flag set to `True`.\n",
    "The output tensor of an operation will require gradients even if only a single input tensor has `requires_grad=True`."
   ],
   "metadata": {
    "id": "DG9b5u7BFvtw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "b = x + z\n",
    "\n",
    "# Task 6.3: Check the gradients of a and b\n",
    "a_grad = a.requires_grad\n",
    "b_grad = b.requires_grad\n",
    "print(f\"Does `a` require gradients?: {a_grad}\")\n",
    "print(f\"Does `b` require gradients?: {b_grad}\")"
   ],
   "metadata": {
    "id": "H52pkrDVFyTo"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Understand dataset creation\n",
    "\n",
    "Write down what functions in torch.utils.data can be used to create a dataset from tensors and load each batch of data for training and testing. (reference https://docs.pytorch.org/docs/stable/data.html)"
   ],
   "metadata": {
    "id": "Zo6GRRqeFnFG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ANSWER"
   ],
   "metadata": {
    "id": "AreAf3z1Frr3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Understand PyTorch models\n",
    "Write down what basic torch.nn function can be used for a linear model. (reference https://docs.pytorch.org/docs/stable/nn.html)"
   ],
   "metadata": {
    "id": "Cl7I20j5Fx2E"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ANSWER"
   ],
   "metadata": {
    "id": "3sNg5HZlF4Yx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Understand optimizers\n",
    "Write down at least two optimizers in torch.optim that are useful to update model weight parameters. (reference https://docs.pytorch.org/docs/main/optim.html)\n"
   ],
   "metadata": {
    "id": "YlqLkzhxF5Kl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Understand loss function\n",
    "Write down two loss functions in torch.nn that can be used for binary classification. (reference https://docs.pytorch.org/docs/stable/nn.html)"
   ],
   "metadata": {
    "id": "AXw4OhpQF-I5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ANSWER"
   ],
   "metadata": {
    "id": "aSkxIF0nGCXw"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
